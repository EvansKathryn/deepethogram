tune:
  use: True
  metrics:
    - val/loss
    - val/f1_class_mean
    - val/data_loss
    - val/reg_loss
    - val/f1_class_mean_nobg
  key_metric: val/f1_class_mean_nobg
  num_trials: 64 # number of runs
  name: tune_feature_extractor
  grace_period: 4 # number of epochs
  search: random # either random or hyperopt
  resources_per_trial:
    gpu: 0.5
    cpu: 3
  hparams:
    # feature_extractor.dropout_p: # each hparam key should be an attribute in a valid configuration
    #   min: 0.0 # min: minimum of range
    #   max: 0.9 # max: maximum of range
    #   space: uniform # space: how to sample
    #   short: dropout # a shortened version to view in Ray's command line interface
    #   current_best: 0.25 # current best estimate. a moving target. used for initializing search space with hyperopt
    train.regularization.alpha: 
      min: 1e-7
      max: 1e-1
      space: log
      short: reg_alpha
      current_best: 0.00045170
    train.regularization.beta: 
      min: 1e-4
      max: 1e-1
      space: log
      short: reg_beta
      current_best: 0.00074703
    train.loss_gamma:
      # choices: [0, 0.5, 1, 2]
      min: 0
      max: 1
      space: uniform
      short: gamma
      current_best: 0.5
    train.loss_weight_exp:
      min: 0.0
      max: 0.5
      space: uniform
      short: loss_weight_exp
      current_best: 0.25
    # train.lr:
    #   min: 1e-5
    #   max: 1e-3
    #   space: log
    #   short: lr
    #   current_best: 1e-4
    feature_extractor.final_bn:
      choices: [True, False]
      space: choice
      short: final_bn
      current_best: True
# use these to overwrite default configuration parameters only when running tune jobs
train:
  viz_examples: False # don't spend time and space making example images 
  steps_per_epoch:
    train: 1000
    val: 1000
  num_epochs: 20
compute:
  metrics_workers: 0 # sometimes has a bug in already multiprocessed jobs
  # batch_size: 64 # auto batch sizing takes a long time